{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"yuDiIJJTBlr9","executionInfo":{"status":"ok","timestamp":1682239125791,"user_tz":-540,"elapsed":3,"user":{"displayName":"구한서 (hanseo)","userId":"04169580702962661413"}}},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"FZ-DjqgWBlsA"},"source":["# ``torch.autograd``를 사용한 자동 미분\n","\n","신경망을 학습할 때 가장 자주 사용되는 알고리즘은 **역전파(back propagation)**이다. 이 알고리즘에서 매개변수(모델 가중치)는 주어진 매개변수에 대한 손실 함수의 **변화도(gradient)**에\n","따라 조정된다.\n","\n","이러한 변화도를 계산하기 위해 PyTorch에는 ``torch.autograd``라고 불리는 자동 미분 엔진이\n","내장되어 있다. 이는 모든 계산 그래프에 대한 변화도의 자동 계산을 지원한다.\n","\n","자동미분을 이해하기 위해서 입력 ``x``, 매개변수 ``w``와 ``b``, 그리고 손실 함수가 있는 매우 간단한 단일 계층 신경망을 가정해보자:\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"QU6TF4o9BlsC","executionInfo":{"status":"ok","timestamp":1682239135591,"user_tz":-540,"elapsed":7832,"user":{"displayName":"구한서 (hanseo)","userId":"04169580702962661413"}}},"outputs":[],"source":["import torch\n","\n","x = torch.ones(5)  # input tensor\n","y = torch.zeros(3)  # expected output\n","w = torch.randn(5, 3, requires_grad=True)\n","b = torch.randn(3, requires_grad=True)\n","z = torch.matmul(x, w)+b\n","loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"]},{"cell_type":"markdown","metadata":{"id":"jDOL-pB7BlsD"},"source":["Tensor, Function과 연산그래프(Computational graph)\n","------------------------------------------------------------------------------------------\n","\n","이 코드는 다음의 **연산 그래프** 를 정의한다:\n","\n","<img src=\"https://pytorch.org/tutorials/_images/comp-graph.png\" width=\"800\" height=\"270\">\n","\n","이 신경망에서, ``w``와 ``b``는 최적화를 해야 하는 **매개변수**이다. 따라서\n","이러한 변수들에 대한 손실 함수의 변화도 $\\frac{\\partial loss}{\\partial w}$와 $\\frac{\\partial loss}{\\partial b}$를 계산할 수 있어야 한다. 이를 위해서 해당 텐서에 대해서는\n","``requires_grad`` 속성을 설정하였다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":347,"status":"ok","timestamp":1679536228333,"user":{"displayName":"권오흠","userId":"05475008821310211864"},"user_tz":-540},"id":"KduHjnLBhqQn","outputId":"c510aa59-0a23-4eff-bed7-e516601cfe61"},"outputs":[{"output_type":"stream","name":"stdout","text":["False\n","False\n","True\n","True\n","True\n","True\n","None\n","None\n","None\n","None\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-2-e9bdd03d105e>:10: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:480.)\n","  print(z.grad)\n","<ipython-input-2-e9bdd03d105e>:11: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:480.)\n","  print(loss.grad)\n"]}],"source":["#requires_grad 속성과 그래디언트 값을 출력하는 코드\n","#requires_grad : 해당 텐서가 그래디언트 계산을 지원하는지 여부를 나타냄 true면 한다\n","#그래디언트 : 텐서에서 계싼된 손실에 대한 기울기 나타냄, 계싼 그래프에서 역방향으로 전파되어 오차를 각 변수에 대해 계싼\n","print(x.requires_grad) #입출력으로 사용, 역전파 계산에 무시\n","print(y.requires_grad)\n","print(w.requires_grad)\n","print(b.requires_grad)\n","print(z.requires_grad)\n","print(loss.requires_grad)\n","\n","print(w.grad)\n","print(b.grad) #아직 역전파 수행하지 않아서 None\n","print(z.grad)\n","print(loss.grad)"]},{"cell_type":"markdown","metadata":{"id":"7nZ7TWvcBlsE"},"source":["**Note:** `requires_grad`의 값은 텐서를 생성할 때 설정하거나, `x.requires_grad_(True)` 메소드를 사용하여 나중에 설정할 수도 있다. `nn.Module`에 속한 패러매터들이나 DataLoader에 의해서 제공되는 입력 텐서들의 `requires_grad` 속성은 자동으로 설정되므로 일반적으로는 신경쓰지 않아도 된다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"URP332hlBlsE"},"source":["연산 그래프를 구성하기 위해 텐서에 적용하는 함수는(예를 들어 `matmul`) 사실 ``Function`` 클래스의 객체이다.\n","이 객체는 순전파 방향으로 함수를 계산하는 방법과, 역방향 전파 단계에서 도함수(derivative)를\n","계산하는 방법을 알고 있다. 역방향 전파 함수에 대한 참조(reference)는 텐서의 ``grad_fn``\n","속성에 저장된다. ``Function``에 대한 자세한 정보는\n","이 [문서](<https://pytorch.org/docs/stable/autograd.html#function>)에서 찾아볼 수 있다.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":274,"status":"ok","timestamp":1679114444320,"user":{"displayName":"권오흠","userId":"05475008821310211864"},"user_tz":-540},"id":"29Dq9B3zBlsF","outputId":"753920aa-9a54-4f6f-b332-68dd68ee72db"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient function for z = <AddBackward0 object at 0x7f04f2dcad30>\n","Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7f04f2dca9a0>\n","Gradient function for w = None\n"]}],"source":["print(f\"Gradient function for z = {z.grad_fn}\")\n","print(f\"Gradient function for loss = {loss.grad_fn}\")\n","\n","print(f\"Gradient function for w = {w.grad_fn}\")"]},{"cell_type":"markdown","metadata":{"id":"w0Pe2TDcBlsF"},"source":["변화도(Gradient) 계산하기\n","-------------------------\n","\n","신경망에서 매개변수의 가중치를 최적화하려면 매개변수에 대한 손실함수의 미분계수(derivative)를\n","계산해야 한다. 즉, 고정된 ``x``와 ``y``에 대해서 $\\frac{\\partial loss}{\\partial w}$와\n","$\\frac{\\partial loss}{\\partial b}$가 필요하다.\n","이러한 도함수를 계산하기 위해, ``loss.backward()`` 를 호출한 다음 ``w.grad``와\n","``b.grad``에서 값을 가져온다:\n","\n",".backward() 메서드는 PyTorch에서 자동 미분 기능을 수행하는 함수 중 하나입니다.\n","\n","주어진 텐서(변수)에 대한 기울기(gradient)를 계산하는 것을 의미합니다. 이 기울기는 해당 텐서를 구성하는 모든 요소들이 손실 함수(loss function)에 대해 미분된 값을 의미합니다. 이 기울기는 역전파 알고리즘을 사용하여 계산됩니다.\n","\n","예를 들어, 모델의 가중치 텐서에 대한 손실 함수의 기울기(gradient)를 계산하고자 한다면, 해당 가중치 텐서가 .requires_grad=True로 설정되어 있어야 하고, 손실 함수의 결과 텐서(스칼라 값)에 .backward() 메서드를 호출하여 해당 텐서에 대한 기울기를 계산할 수 있습니다.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":311,"status":"ok","timestamp":1679114450737,"user":{"displayName":"권오흠","userId":"05475008821310211864"},"user_tz":-540},"id":"91FLKS58BlsG","outputId":"f035f0ae-9324-446f-f6b9-aa82118a5cd5"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2288, 0.2604, 0.3223],\n","        [0.2288, 0.2604, 0.3223],\n","        [0.2288, 0.2604, 0.3223],\n","        [0.2288, 0.2604, 0.3223],\n","        [0.2288, 0.2604, 0.3223]])\n","tensor([0.2288, 0.2604, 0.3223])\n"]}],"source":["loss.backward()\n","print(w.grad)\n","print(b.grad)"]},{"cell_type":"markdown","metadata":{"id":"_Jyv1hHbBlsG"},"source":["**Note:** \n","  - 연산 그래프의 잎(leaf) 노드들 중 ``requires_grad`` 속성이 ``True``로 설정된\n","    노드들의 ``grad`` 속성만 구할 수 있다. 다른 노드들에서는 변화도를 계산할 수 없다.\n","  - 성능 상의 이유로, 주어진 그래프에서의 ``backward``를 사용한 변화도 계산은 한 번만\n","    수행할 수 있다. 만약 동일한 그래프에서 여러번의 ``backward`` 호출이 필요하면,\n","    ``backward`` 호출 시에 ``retrain_graph=True``를 전달해야 한다."]},{"cell_type":"markdown","metadata":{"id":"z5xy6t3eBlsG"},"source":["변화도 추적 멈추기\n","------------------------------------------------------------------------------------------\n","\n","기본적으로, ``requires_grad=True``인 모든 텐서들은 연산 기록을 추적하고 변화도 계산을\n","지원한다. 그러나 모델을 학습한 뒤 입력 데이터를 단순히 적용하기만 하는 경우와 같이 순전파\n","연산만 필요한 경우에는, 이러한 추적이나 지원이 필요없을 수 있다.\n","연산 코드를 ``torch.no_grad()`` 블록으로 둘러싸서 연산 추적을 멈출 수 있다:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":286,"status":"ok","timestamp":1679114474993,"user":{"displayName":"권오흠","userId":"05475008821310211864"},"user_tz":-540},"id":"kyIRJeSMBlsH","outputId":"40d31ecb-c264-4e46-b3bf-f707ae10f210"},"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","False\n"]}],"source":["z = torch.matmul(x, w)+b\n","print(z.requires_grad)\n","\n","with torch.no_grad():\n","    z = torch.matmul(x, w)+b\n","print(z.requires_grad)"]},{"cell_type":"markdown","metadata":{"id":"24pNecWzBlsI"},"source":["동일한 결과를 얻는 다른 방법은 텐서에 ``detach()`` 메소드를 사용하는 것이다:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":286,"status":"ok","timestamp":1679114479120,"user":{"displayName":"권오흠","userId":"05475008821310211864"},"user_tz":-540},"id":"rNdi1SuWBlsI","outputId":"6087a95a-a2ce-4308-ccaa-72ce87b62404"},"outputs":[{"output_type":"stream","name":"stdout","text":["False\n"]}],"source":["z = torch.matmul(x, w)+b\n","z_det = z.detach()\n","print(z_det.requires_grad)"]},{"cell_type":"markdown","metadata":{"id":"vf6cfsZFBlsJ"},"source":["변화도 추적을 멈춰야 하는 이유들은 다음과 같다:\n","  - 신경망의 일부 매개변수를 **고정된 매개변수(frozen parameter)**로 만들고 싶은 경우이다. 이는\n","    사전 학습된 신경망을 [미세조정(fine tuning)](<https://tutorials.pytorch.kr/beginner/finetuning_torchvision_models_tutorial.html>)\n","    할 때 매우 일반적인 시나리오이다.\n","  - 변화도를 추적하지 않는 텐서의 연산이 더 효율적이기 때문에, 순전파 단계만 수행할 때\n","    **연산 속도가 향상된다.**\n","    #역전파일때 기울기 계싼 못 하게 되기때문에 변화도 추적하는것이 바람직하다\n","\n"]},{"cell_type":"markdown","metadata":{"id":"q1QhBH90BlsJ"},"source":["연산 그래프\n","------------------------------------------------------------------------------------------\n","\n","개념적으로, autograd는 데이터(텐서), 실행된 모든 연산들, 그리고 연산의 결과인 새로운 텐서에 대한\n","기록을 [`Function`](<https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function>) 객체들로 구성된 방향성 비순환 그래프(DAG; Directed Acyclic Graph)에 저장한다.\n","이 DAG의 잎(leave)은 입력 텐서이고, 뿌리(root)는 출력 텐서이다.\n","이 그래프를 뿌리에서부터 잎까지 추적하면 연쇄 법칙(chain rule)에 따라 변화도를 자동으로 계산할 수 있다.\n","\n","순전파 단계에서, autograd는 다음 두 가지 작업을 동시에 수행한다:\n","\n","- 요청된 연산을 수행하여 결과 텐서를 계산하고,\n","- DAG에 연산의 *변화도 함수(gradient function)*를 유지(maintain)한다.\n","\n","역전파 단계는 ``.backward()``가 호출될 때 DAG의 뿌리(root)에서 시작된다. ``autograd``는 이 때:\n","\n","- 각 ``.grad_fn``으로부터 변화도를 계산하고,\n","- 각 텐서의 ``.grad`` 속성에 계산 결과를 누적하고(accumulate),\n","- 연쇄 법칙을 사용하여, 모든 잎(leaf) 텐서들까지 전파(propagate)한다.\n","\n","\n","**Note:** **PyTorch에서 DAG들은 동적(dynamic)이다.**\n","주목해야 할 중요한 점은 그래프가 처음부터(from scratch) 다시 생성된다는 것이다; 매번 ``.bachward()``가 호출되고 나면, autograd는 새로운 그래프를 만들기(populate) 시작한다. 이러한 점 덕분에 모델에서 흐름 제어(control flow) 구문들을 사용할 수 있게 되는 것이다. 매번 반복(iteration)할 때마다 필요하면 모양(shape)이나 크기(size), 연산(operation)을 바꿀 수도 있다.\n","순전파->역전파->가중치,편향 조정->다음 순전파\n"]},{"cell_type":"markdown","metadata":{"id":"gO5FolLtBlsK"},"source":["텐서 변화도와 야코비안 곱 (Jacobian Product)\n","------------------------------------------------------------------------------------------\n","\n","대부분의 경우에 신경망에서 손실함수는 스칼라 함수이다.\n","그러나 손실 함수의 출력이 임의의 텐서인 경우가 있다. 이럴 때, PyTorch는 실제 변화도가 아닌\n","**야코비안 곱(Jacobian product)**을 계산한다.\n","\n","$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$이고,\n","$\\vec{y}=\\langle y_1,\\dots,y_m\\rangle$일 때\n","벡터 함수 $\\vec{y}=f(\\vec{x})$에서 $\\vec{x}$에 대한\n","$\\vec{y}$의 변화도는 **야코비안 행렬(Jacobian matrix)**로 주어진다:\n","\n","\\begin{align}J=\\left(\\begin{array}{ccc}\n","      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n","      \\vdots & \\ddots & \\vdots\\\\\n","      \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n","      \\end{array}\\right)\\end{align}\n","\n","야코비안 행렬 자체를 계산하는 대신, PyTorch는 주어진 입력 벡터 $v=(v_1 \\dots v_m)$에 대한\n","**야코비안 곱(Jacobian Product)**  $v^T\\cdot J$을 계산한다.\n","이 과정은 $v$를 인자로 ``backward``를 호출하면 이루어진다.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":290,"status":"ok","timestamp":1679537632243,"user":{"displayName":"권오흠","userId":"05475008821310211864"},"user_tz":-540},"id":"DysZ46VqBlsL","outputId":"02a5dd0f-4eef-4f1e-cc6b-07d55677d2f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[4., 1., 1., 1., 1.],\n","        [1., 4., 1., 1., 1.],\n","        [1., 1., 4., 1., 1.],\n","        [1., 1., 1., 4., 1.],\n","        [1., 1., 1., 1., 4.]], grad_fn=<PowBackward0>)\n","First call\n","tensor([[4., 2., 2., 2., 2.],\n","        [2., 4., 2., 2., 2.],\n","        [2., 2., 4., 2., 2.],\n","        [2., 2., 2., 4., 2.],\n","        [2., 2., 2., 2., 4.]])\n","\n","Second call\n","tensor([[8., 4., 4., 4., 4.],\n","        [4., 8., 4., 4., 4.],\n","        [4., 4., 8., 4., 4.],\n","        [4., 4., 4., 8., 4.],\n","        [4., 4., 4., 4., 8.]])\n","\n","Call after zeroing gradients\n","tensor([[4., 2., 2., 2., 2.],\n","        [2., 4., 2., 2., 2.],\n","        [2., 2., 4., 2., 2.],\n","        [2., 2., 2., 4., 2.],\n","        [2., 2., 2., 2., 4.]])\n"]}],"source":["inp = torch.eye(5, requires_grad=True)\n","out = (inp+1).pow(2)\n","print(out)\n","out.backward(torch.ones_like(inp), retain_graph=True)\n","print(f\"First call\\n{inp.grad}\")\n","\n","out.backward(torch.ones_like(inp), retain_graph=True)\n","print(f\"\\nSecond call\\n{inp.grad}\")\n","inp.grad.zero_()\n","out.backward(torch.ones_like(inp), retain_graph=True)\n","print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"]},{"cell_type":"markdown","metadata":{"id":"peuQsWWNBlsL"},"source":["동일한 인자로 ``backward``를 두차례 호출하면 변화도 값이 달라진다.\n","이는 역방향 전파를 수행할 때, PyTorch가 **변화도를 누적(accumulate)해두기 때문**\n","이다. 즉, 계산된 변화도의 값이 연산 그래프의 모든 잎(leaf) 노드의 ``grad`` 속성에\n","더해진다. 따라서 제대로된 변화도를 계산하기 위해서는 ``grad`` 속성을 먼저 0으로 만들어야\n","한다. 실제 학습 과정에서는 **옵티마이저(optimizer)**가 이 과정을 도와준다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IXHdGI7gBlsM"},"source":["**Note:** 이전에는 매개변수 없이 ``backward()`` 함수를 호출했다. 이는 본질적으로\n","``backward(torch.tensor(1.0))``을 호출하는 것과 동일하며,\n","신경망 훈련 중의 손실과 같은 스칼라-값 함수의 변화도를 계산하는 유용한 방법이다.\n"]},{"cell_type":"markdown","metadata":{"id":"os5YWxSqBlsM"},"source":["--------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NcwcXWOeBlsM"},"source":["### 더 읽어보기\n","\n","- [Autograd Mechanics](<https://pytorch.org/docs/stable/notes/autograd.html>)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1easXVKjlM0vjzgOUhBb6fpDM0OjLjxtW","timestamp":1680565472452}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":0}